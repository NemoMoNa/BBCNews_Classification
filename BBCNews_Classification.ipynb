{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f33a1b5e-b3ba-4c26-b621-be31ad0db805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_DIR: /Users/mh/Downloads/Mini Project/Dec30_BBCNews\n",
      "RESULTS_DIR: /Users/mh/Downloads/Mini Project/Dec30_BBCNews/results\n",
      "MODELS_DIR : /Users/mh/Downloads/Mini Project/Dec30_BBCNews/models\n",
      "LOGS_DIR   : /Users/mh/Downloads/Mini Project/Dec30_BBCNews/logs\n"
     ]
    }
   ],
   "source": [
    "#Cell 0: フォルダ作成\n",
    "import os, time, random\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "\n",
    "PROJECT_DIR = \"/Users/mh/Downloads/Mini Project/Dec30_BBCNews\"\n",
    "RESULTS_DIR = os.path.join(PROJECT_DIR, \"results\")\n",
    "MODELS_DIR  = os.path.join(PROJECT_DIR, \"models\")\n",
    "LOGS_DIR    = os.path.join(PROJECT_DIR, \"logs\")\n",
    "\n",
    "for d in [RESULTS_DIR, MODELS_DIR, LOGS_DIR]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "print(\"PROJECT_DIR:\", PROJECT_DIR)\n",
    "print(\"RESULTS_DIR:\", RESULTS_DIR)\n",
    "print(\"MODELS_DIR :\", MODELS_DIR)\n",
    "print(\"LOGS_DIR   :\", LOGS_DIR)\n",
    "\n",
    "def nowstamp():\n",
    "    return datetime.now().strftime(\"%Y%m%d_%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac9674e0-4138-498a-b0a6-6bf3c43d5b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BBCNewsConfig(model_name='distilbert-base-uncased', dataset_name='SetFit/bbc-news', text_col='text', label_col='label', max_len=256, batch_size=16, lr=2e-05, epochs=3, seed=42, max_train_examples=8000, max_val_examples=1000, max_test_examples=2000, debug_max_steps_per_epoch=300, early_stopping_patience=1, results_dir='/Users/mh/Downloads/Mini Project/Dec30_BBCNews/results', models_dir='/Users/mh/Downloads/Mini Project/Dec30_BBCNews/models', logs_dir='/Users/mh/Downloads/Mini Project/Dec30_BBCNews/logs')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cell 1: Imports & Config\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "# --- device ---\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "\n",
    "@dataclass\n",
    "class BBCNewsConfig:\n",
    "    model_name: str = \"distilbert-base-uncased\"\n",
    "    dataset_name: str = \"SetFit/bbc-news\"   # ← bbc-news\n",
    "    text_col: str = \"text\"\n",
    "    label_col: str = \"label\"\n",
    "\n",
    "    max_len: int = 256\n",
    "    batch_size: int = 16\n",
    "    lr: float = 2e-5\n",
    "    epochs: int = 3\n",
    "    seed: int = 42\n",
    "\n",
    "    # 軽量化（Macでも回る）\n",
    "    max_train_examples: int = 8000\n",
    "    max_val_examples: int = 1000\n",
    "    max_test_examples: int = 2000\n",
    "\n",
    "    # デバッグ（1epochの最大step数。Noneで無制限）\n",
    "    debug_max_steps_per_epoch: int | None = 300\n",
    "\n",
    "    # early stopping\n",
    "    early_stopping_patience: int = 1\n",
    "\n",
    "    # 出力先\n",
    "    results_dir: str = RESULTS_DIR\n",
    "    models_dir: str = MODELS_DIR\n",
    "    logs_dir: str = LOGS_DIR\n",
    "\n",
    "cfg = BBCNewsConfig()\n",
    "cfg\n",
    "#print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fe5d2b7-5cc1-42e8-a42f-fc9d8a570841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', 'label_text'],\n",
      "        num_rows: 1225\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label', 'label_text'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n",
      "train: 1102 val: 123 test: 1000\n",
      "{'text': 'wales want rugby league training wales could follow england s lead by training with a rugby league club.  england have already had a three-day session with leeds rhinos  and wales are thought to be interested in a similar clinic with rivals st helens. saints coach ian millward has given his approval  but if it does happen it is unlikely to be this season. saints have a week s training in portugal next week  while wales will play england in the opening six nations match on 5 february.  we have had an approach from wales   confirmed a saints spokesman.  it s in the very early stages but it is something we are giving serious consideration to.  st helens  who are proud of their welsh connections  are obvious partners for the welsh rugby union  despite a spat in 2001 over the collapse of kieron cunningham s proposed £500 000 move to union side swansea. a similar cross-code deal that took iestyn harris from leeds to cardiff in 2001 did go through  before the talented stand-off returned to the 13-man code with bradford bulls. kel coslett  who famously moved from wales to league in the 1960s  is currently saints  football manager  while clive griffiths - wales  defensive coach - is a former st helens player and is thought to be the man behind the latest initiative. scott gibbs  the former wales and lions centre  played for st helens from 1994-96 and was in the challenge cup-winning team at wembley in 1996.', 'label': 2, 'label_text': 'sport'}\n"
     ]
    }
   ],
   "source": [
    "#Cell 2: Seed固定 & Datasetロード + 分割\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(cfg.seed)\n",
    "\n",
    "ds = load_dataset(cfg.dataset_name)\n",
    "print(ds)\n",
    "\n",
    "# SetFit/bbc-news は train / test があることが多い。なければ自前split。\n",
    "if \"train\" in ds and \"test\" in ds:\n",
    "    train_full = ds[\"train\"].shuffle(seed=cfg.seed)\n",
    "    test_ds    = ds[\"test\"].shuffle(seed=cfg.seed)\n",
    "else:\n",
    "    # 念のため fallback\n",
    "    full = ds[list(ds.keys())[0]].shuffle(seed=cfg.seed)\n",
    "    tmp = full.train_test_split(test_size=0.2, seed=cfg.seed)\n",
    "    train_full, test_ds = tmp[\"train\"], tmp[\"test\"]\n",
    "\n",
    "# train -> train/val\n",
    "split = train_full.train_test_split(test_size=0.1, seed=cfg.seed)\n",
    "train_ds, val_ds = split[\"train\"], split[\"test\"]\n",
    "\n",
    "# 軽量化（必要なら）\n",
    "def take_n(d, n):\n",
    "    if n is None: \n",
    "        return d\n",
    "    n = min(n, len(d))\n",
    "    return d.select(range(n))\n",
    "\n",
    "train_ds = take_n(train_ds, cfg.max_train_examples)\n",
    "val_ds   = take_n(val_ds, cfg.max_val_examples)\n",
    "test_ds  = take_n(test_ds, cfg.max_test_examples)\n",
    "\n",
    "print(\"train:\", len(train_ds), \"val:\", len(val_ds), \"test:\", len(test_ds))\n",
    "print(train_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cd5af59-e5e2-492a-824c-e351fc2bdba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': tensor(2),\n",
       " 'input_ids': tensor([  101,  3575,  2215,  4043,  2223,  2731,  3575,  2071,  3582,  2563,\n",
       "          1055,  2599,  2011,  2731,  2007,  1037,  4043,  2223,  2252,  1012,\n",
       "          2563,  2031,  2525,  2018,  1037,  2093,  1011,  2154,  5219,  2007,\n",
       "          7873, 24091,  2015,  1998,  3575,  2024,  2245,  2000,  2022,  4699,\n",
       "          1999,  1037,  2714,  9349,  2007,  9169,  2358, 24074,  1012,  6586,\n",
       "          2873,  4775,  4971,  7652,  2038,  2445,  2010,  6226,  2021,  2065,\n",
       "          2009,  2515,  4148,  2009,  2003,  9832,  2000,  2022,  2023,  2161,\n",
       "          1012,  6586,  2031,  1037,  2733,  1055,  2731,  1999,  5978,  2279,\n",
       "          2733,  2096,  3575,  2097,  2377,  2563,  1999,  1996,  3098,  2416,\n",
       "          3741,  2674,  2006,  1019,  2337,  1012,  2057,  2031,  2018,  2019,\n",
       "          3921,  2013,  3575,  4484,  1037,  6586, 14056,  1012,  2009,  1055,\n",
       "          1999,  1996,  2200,  2220,  5711,  2021,  2009,  2003,  2242,  2057,\n",
       "          2024,  3228,  3809,  9584,  2000,  1012,  2358, 24074,  2040,  2024,\n",
       "          7098,  1997,  2037,  6124,  7264,  2024,  5793,  5826,  2005,  1996,\n",
       "          6124,  4043,  2586,  2750,  1037, 14690,  1999,  2541,  2058,  1996,\n",
       "          7859,  1997, 11382, 26534, 13652,  1055,  3818, 27813,  8889,  2199,\n",
       "          2693,  2000,  2586,  2217, 16085,  1012,  1037,  2714,  2892,  1011,\n",
       "          3642,  3066,  2008,  2165, 29464, 21756,  2078,  5671,  2013,  7873,\n",
       "          2000, 10149,  1999,  2541,  2106,  2175,  2083,  2077,  1996, 10904,\n",
       "          3233,  1011,  2125,  2513,  2000,  1996,  2410,  1011,  2158,  3642,\n",
       "          2007,  9999, 12065,  1012, 17710,  2140,  2522, 25016,  4779,  2040,\n",
       "         18172,  2333,  2013,  3575,  2000,  2223,  1999,  1996,  4120,  2003,\n",
       "          2747,  6586,  2374,  3208,  2096, 14675, 21960,  1011,  3575,  5600,\n",
       "          2873,  1011,  2003,  1037,  2280,  2358, 24074,  2447,  1998,  2003,\n",
       "          2245,  2000,  2022,  1996,  2158,  2369,  1996,  6745,  6349,  1012,\n",
       "          3660, 15659,  1996,  2280,  3575,   102]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cell 3: Tokenizer & Tokenize（-100はここでは不要）\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model_name, use_fast=True)\n",
    "\n",
    "def tokenize_batch(batch):\n",
    "    return tokenizer(\n",
    "        batch[cfg.text_col],\n",
    "        truncation=True,\n",
    "        max_length=cfg.max_len,\n",
    "    )\n",
    "\n",
    "train_tok = train_ds.map(tokenize_batch, batched=True, remove_columns=[cfg.text_col])\n",
    "val_tok   = val_ds.map(tokenize_batch, batched=True, remove_columns=[cfg.text_col])\n",
    "test_tok  = test_ds.map(tokenize_batch, batched=True, remove_columns=[cfg.text_col])\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "cols = [\"input_ids\", \"attention_mask\", cfg.label_col]\n",
    "train_tok.set_format(type=\"torch\", columns=cols)\n",
    "val_tok.set_format(type=\"torch\", columns=cols)\n",
    "test_tok.set_format(type=\"torch\", columns=cols)\n",
    "\n",
    "train_tok[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7516b38a-3844-475a-b448-2f2f8862360e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': torch.Size([16, 256]),\n",
       " 'attention_mask': torch.Size([16, 256]),\n",
       " 'labels': torch.Size([16])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cell 4: DataLoader\n",
    "train_loader = DataLoader(\n",
    "    train_tok,\n",
    "    batch_size=cfg.batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_tok,\n",
    "    batch_size=cfg.batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_tok,\n",
    "    batch_size=cfg.batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "{k: (v.shape if hasattr(v, \"shape\") else type(v)) for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f9b7006-e42c-4c91-bef0-edb564a711e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique_labels: [0, 1, 2, 3, 4]\n",
      "num_labels: 5\n",
      "label_names: ['0', '1', '2', '3', '4']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#Cell 5: Model & Optimizer\n",
    "import numpy as np\n",
    "\n",
    "# label は Value(int) なので num_classes は無い → データから数える\n",
    "unique_labels = sorted(set(train_ds[cfg.label_col]))\n",
    "num_labels = len(unique_labels)\n",
    "\n",
    "print(\"unique_labels:\", unique_labels)\n",
    "print(\"num_labels:\", num_labels)\n",
    "\n",
    "# label_names も自前で作る（今回は 0..N-1 をそのまま名前にする）\n",
    "label_names = [str(i) for i in unique_labels]\n",
    "print(\"label_names:\", label_names)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    cfg.model_name,\n",
    "    num_labels=num_labels,\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=cfg.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f50b8aea-d3fb-43c4-a336-7586a142eff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 6: eval_model\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Cell X: eval_model (with key check + robust label key handling)\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import torch\n",
    "\n",
    "def eval_model(model, dataloader):\n",
    "    \"\"\"\n",
    "    returns: avg_loss, acc, f1_macro, all_labels, all_preds\n",
    "    \"\"\"\n",
    "\n",
    "    # 0) （デバッグ）dataloader が返す batch のキーを 1回だけ確認する\n",
    "    #    ※重い処理ではないが、毎回の評価で何度も表示したくないので try/except にする\n",
    "    try:\n",
    "        first_batch = next(iter(dataloader))\n",
    "        print(\"[eval_model] batch keys:\", first_batch.keys())\n",
    "    except StopIteration:\n",
    "        raise ValueError(\"dataloader is empty (no batches).\")\n",
    "\n",
    "    # 1) model を eval モードにする\n",
    "    model.eval()\n",
    "\n",
    "    # 2) 正解ラベル・予測ラベルを入れるリストを用意する\n",
    "    all_labels = []\n",
    "    all_preds  = []\n",
    "\n",
    "    # 3) total_loss と n_steps を初期化する\n",
    "    total_loss = 0.0\n",
    "    n_steps = 0\n",
    "\n",
    "    # 4) 勾配計算をオフにして dataloader をループする\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # 4-1) バッチを DEVICE に転送する\n",
    "            batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "\n",
    "            # 4-2) model(**batch) で出力を得る\n",
    "            out = model(**batch)\n",
    "\n",
    "            # 4-3) loss を蓄積し、ステップ数を数える\n",
    "            loss = out.loss\n",
    "            total_loss += float(loss.item())\n",
    "            n_steps += 1\n",
    "\n",
    "            # 4-4) logits から予測ラベル（argmax）を計算する\n",
    "            logits = out.logits\n",
    "            preds = logits.argmax(dim=-1)\n",
    "\n",
    "            # 4-5) labels キー名の揺れ（labels / label / cfg.label_col）を吸収して取り出す\n",
    "            if \"cfg\" in globals() and hasattr(cfg, \"label_col\") and cfg.label_col in batch:\n",
    "                labels = batch[cfg.label_col]\n",
    "            elif \"labels\" in batch:\n",
    "                labels = batch[\"labels\"]\n",
    "            elif \"label\" in batch:\n",
    "                labels = batch[\"label\"]\n",
    "            else:\n",
    "                raise KeyError(f\"batch has no labels key. keys={list(batch.keys())}\")\n",
    "\n",
    "            # 4-6) CPU に戻して list に追加する\n",
    "            all_labels.extend(labels.detach().cpu().numpy().tolist())\n",
    "            all_preds.extend(preds.detach().cpu().numpy().tolist())\n",
    "\n",
    "    # 5) 平均 loss を計算する\n",
    "    avg_loss = total_loss / max(1, n_steps)\n",
    "\n",
    "    # 6) accuracy と macro F1 を計算する\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1  = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "\n",
    "    # 7) (avg_loss, acc, f1, all_labels, all_preds) を返す\n",
    "    return avg_loss, acc, f1, all_labels, all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd29efb3-d7a4-4bd8-a711-29021954dc57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save dir: /Users/mh/Downloads/Mini Project/Dec30_BBCNews/models/distilbert_bbcnews_20260104_143108\n",
      "Log   : /Users/mh/Downloads/Mini Project/Dec30_BBCNews/logs/train_log_20260104_143108.tsv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7862b5c0d4e347a28b67a4782a8a92e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/3:   0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[eval_model] batch keys: KeysView({'input_ids': tensor([[  101, 10687,  2753,  ...,  2718,  2774,   102],\n",
      "        [  101, 17235,  2850,  ...,     0,     0,     0],\n",
      "        [  101,  7436, 17853,  ..., 18510,  4626,   102],\n",
      "        ...,\n",
      "        [  101,  3617, 11972,  ...,  2241,  2006,   102],\n",
      "        [  101,  3163,  2538,  ...,  2272,  2117,   102],\n",
      "        [  101,  6261,  2557,  ..., 14534,  2005,   102]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([3, 1, 3, 0, 3, 1, 3, 3, 4, 3, 3, 2, 1, 0, 2, 0])})\n",
      "\n",
      "Epoch 1/3 steps=69 train_loss=0.8598 val_loss=0.1928 val_acc=0.9919 val_f1=0.9904\n",
      "  -> best model updated & saved.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08ae591537ba4c99bdfeaefdd0a71305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/3:   0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[eval_model] batch keys: KeysView({'input_ids': tensor([[  101, 10687,  2753,  ...,  2718,  2774,   102],\n",
      "        [  101, 17235,  2850,  ...,     0,     0,     0],\n",
      "        [  101,  7436, 17853,  ..., 18510,  4626,   102],\n",
      "        ...,\n",
      "        [  101,  3617, 11972,  ...,  2241,  2006,   102],\n",
      "        [  101,  3163,  2538,  ...,  2272,  2117,   102],\n",
      "        [  101,  6261,  2557,  ..., 14534,  2005,   102]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([3, 1, 3, 0, 3, 1, 3, 3, 4, 3, 3, 2, 1, 0, 2, 0])})\n",
      "\n",
      "Epoch 2/3 steps=69 train_loss=0.1187 val_loss=0.0556 val_acc=0.9919 val_f1=0.9904\n",
      "  -> best model updated & saved.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d41ab97e81144e5b562bd194e0ac768",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/3:   0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[eval_model] batch keys: KeysView({'input_ids': tensor([[  101, 10687,  2753,  ...,  2718,  2774,   102],\n",
      "        [  101, 17235,  2850,  ...,     0,     0,     0],\n",
      "        [  101,  7436, 17853,  ..., 18510,  4626,   102],\n",
      "        ...,\n",
      "        [  101,  3617, 11972,  ...,  2241,  2006,   102],\n",
      "        [  101,  3163,  2538,  ...,  2272,  2117,   102],\n",
      "        [  101,  6261,  2557,  ..., 14534,  2005,   102]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([3, 1, 3, 0, 3, 1, 3, 3, 4, 3, 3, 2, 1, 0, 2, 0])})\n",
      "\n",
      "Epoch 3/3 steps=69 train_loss=0.0324 val_loss=0.0522 val_acc=0.9919 val_f1=0.9904\n",
      "  -> best model updated & saved.\n"
     ]
    }
   ],
   "source": [
    "#Cell 7: Training loop（logging + early stopping）\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "ts = nowstamp()\n",
    "run_name = f\"distilbert_bbcnews_{ts}\"\n",
    "save_dir = os.path.join(cfg.models_dir, run_name)\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "log_path = os.path.join(cfg.logs_dir, f\"train_log_{ts}.tsv\")\n",
    "\n",
    "print(\"Save dir:\", save_dir)\n",
    "print(\"Log   :\", log_path)\n",
    "\n",
    "with open(log_path, \"w\") as f:\n",
    "    f.write(\"epoch\\tsteps\\ttrain_loss\\tval_loss\\tval_acc\\tval_f1\\n\")\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for ep in range(1, cfg.epochs + 1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    n_steps = 0\n",
    "\n",
    "    progress = tqdm(train_loader, total=len(train_loader), desc=f\"Epoch {ep}/{cfg.epochs}\")\n",
    "\n",
    "    for step, batch in enumerate(progress): # enumerate()「今、何番目の処理か」\n",
    "        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(**batch)\n",
    "        loss = out.loss\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        n_steps += 1\n",
    "        progress.set_postfix({\"loss\": float(loss.item())})\n",
    "\n",
    "        if cfg.debug_max_steps_per_epoch is not None and (step + 1) >= cfg.debug_max_steps_per_epoch:\n",
    "            break\n",
    "\n",
    "    train_loss = total_loss / max(1, n_steps)\n",
    "    val_loss, val_acc, val_f1, _, _ = eval_model(model, val_loader)\n",
    "\n",
    "    if DEVICE == \"mps\":\n",
    "        torch.mps.empty_cache()\n",
    "\n",
    "    print(f\"\\nEpoch {ep}/{cfg.epochs} steps={n_steps} \"\n",
    "          f\"train_loss={train_loss:.4f} val_loss={val_loss:.4f} \"\n",
    "          f\"val_acc={val_acc:.4f} val_f1={val_f1:.4f}\")\n",
    "\n",
    "    with open(log_path, \"a\") as f:\n",
    "        f.write(f\"{ep}\\t{n_steps}\\t{train_loss:.6f}\\t{val_loss:.6f}\\t{val_acc:.6f}\\t{val_f1:.6f}\\n\")\n",
    "\n",
    "    # Early stopping & best model save\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "        model.save_pretrained(save_dir)\n",
    "        tokenizer.save_pretrained(save_dir)\n",
    "        print(\"  -> best model updated & saved.\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"  -> no improvement ({epochs_no_improve}/{cfg.early_stopping_patience})\")\n",
    "        if epochs_no_improve >= cfg.early_stopping_patience:\n",
    "            print(\"  -> Early stopping triggered.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30928696-7c6b-439c-9f0c-1d5994b5ad60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[eval_model] batch keys: KeysView({'input_ids': tensor([[  101,  3956,  3504,  ...,  2006,  3956,   102],\n",
      "        [  101,  3782,  6473,  ...,  2793,  2790,   102],\n",
      "        [  101,  2844,  5157,  ...,  2000,  1002,   102],\n",
      "        ...,\n",
      "        [  101,  9580, 21208,  ...,  2009,  1999,   102],\n",
      "        [  101,  7842, 16294,  ...,  2052,  2663,   102],\n",
      "        [  101,  6172,  6925,  ...,  4791,  2013,   102]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([1, 3, 1, 4, 0, 1, 4, 0, 2, 4, 4, 0, 2, 0, 2, 3])})\n",
      "TEST  loss=0.1214 acc=0.9720 f1=0.9716\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.98      0.96       189\n",
      "           1       0.96      0.96      0.96       224\n",
      "           2       1.00      1.00      1.00       236\n",
      "           3       1.00      0.96      0.98       176\n",
      "           4       0.97      0.97      0.97       175\n",
      "\n",
      "    accuracy                           0.97      1000\n",
      "   macro avg       0.97      0.97      0.97      1000\n",
      "weighted avg       0.97      0.97      0.97      1000\n",
      "\n",
      "Saved: /Users/mh/Downloads/Mini Project/Dec30_BBCNews/results/confusion_matrix_20260104_143108.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>185</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>214</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>235</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>169</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4\n",
       "0  185    3    1    0    0\n",
       "1    6  214    0    0    4\n",
       "2    0    1  235    0    0\n",
       "3    5    1    0  169    1\n",
       "4    1    5    0    0  169"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cell 8: Test評価 + Confusion Matrix + errors.tsv 保存\n",
    "# best model をロード（念のため）\n",
    "model = AutoModelForSequenceClassification.from_pretrained(save_dir).to(DEVICE)\n",
    "\n",
    "test_loss, test_acc, test_f1, y_true, y_pred = eval_model(model, test_loader)\n",
    "print(f\"TEST  loss={test_loss:.4f} acc={test_acc:.4f} f1={test_f1:.4f}\")\n",
    "\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=label_names))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "cm_df = pd.DataFrame(cm, index=label_names, columns=label_names)\n",
    "cm_path = os.path.join(cfg.results_dir, f\"confusion_matrix_{ts}.csv\")\n",
    "cm_df.to_csv(cm_path)\n",
    "print(\"Saved:\", cm_path)\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d81f5f1-227e-4305-9288-fa2659ef4d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /Users/mh/Downloads/Mini Project/Dec30_BBCNews/results/errors_20260104_143108.tsv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>true_id</th>\n",
       "      <th>pred_id</th>\n",
       "      <th>true_label</th>\n",
       "      <th>pred_label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>pc ownership to  double by 2010  the number of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>74</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>music mogul fuller sells company pop idol supr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>109</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>fast lifts rise into record books two high-spe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>119</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>baa support ahead of court battle uk airport o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>214</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>ban on forced retirement under 65 employers wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>256</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>eu fraud clampdown urged eu member states are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>265</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>orange colour clash set for court a row over t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>267</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>new media battle for bafta awards the bbc lead...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>329</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>piero gives rugby perspective bbc sport unveil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>335</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>pupils to get anti-piracy lessons lessons on m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx  true_id  pred_id true_label pred_label  \\\n",
       "0   36        0        1          0          1   \n",
       "1   74        3        1          3          1   \n",
       "2  109        0        1          0          1   \n",
       "3  119        4        1          4          1   \n",
       "4  214        1        4          1          4   \n",
       "5  256        4        1          4          1   \n",
       "6  265        1        0          1          0   \n",
       "7  267        3        0          3          0   \n",
       "8  329        0        2          0          2   \n",
       "9  335        3        0          3          0   \n",
       "\n",
       "                                                text  \n",
       "0  pc ownership to  double by 2010  the number of...  \n",
       "1  music mogul fuller sells company pop idol supr...  \n",
       "2  fast lifts rise into record books two high-spe...  \n",
       "3  baa support ahead of court battle uk airport o...  \n",
       "4  ban on forced retirement under 65 employers wi...  \n",
       "5  eu fraud clampdown urged eu member states are ...  \n",
       "6  orange colour clash set for court a row over t...  \n",
       "7  new media battle for bafta awards the bbc lead...  \n",
       "8  piero gives rugby perspective bbc sport unveil...  \n",
       "9  pupils to get anti-piracy lessons lessons on m...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# errors.tsv（誤分類を保存）\n",
    "# 元データ（test_ds）からテキストも引っ張る\n",
    "errors = []\n",
    "for i, (yt, yp) in enumerate(zip(y_true, y_pred)):\n",
    "    if yt != yp:\n",
    "        ex = test_ds[i]\n",
    "        errors.append({\n",
    "            \"idx\": i,\n",
    "            \"true_id\": yt,\n",
    "            \"pred_id\": yp,\n",
    "            \"true_label\": label_names[yt],\n",
    "            \"pred_label\": label_names[yp],\n",
    "            \"text\": ex[cfg.text_col],\n",
    "        })\n",
    "\n",
    "err_df = pd.DataFrame(errors)\n",
    "err_path = os.path.join(cfg.results_dir, f\"errors_{ts}.tsv\")\n",
    "err_df.to_csv(err_path, sep=\"\\t\", index=False)\n",
    "print(\"Saved:\", err_path)\n",
    "err_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea11e13-7ec6-403c-88db-71e2162f3c2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ai-train-2025)",
   "language": "python",
   "name": "ai-train-2025"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
